# ðŸ”´ Issue Analysis: AI Responses Not Working

## Problem

The chat application is displaying **saved conversation history** instead of generating **real AI responses** from MedGemma.

### Root Causes Identified

1. **Wrong LLM Integration Path**
   - The system tries to use `OpenAI` wrapper first (via `chat.ai` module)
   - When OpenAI isn't available, it falls back to `generate_bot_response_fallback()`
   - **MedGemma is never called** - it exists only in separate endpoints (`/api/medgemma/text/`)

2. **Two Separate AI Integration Paths**
   - **Path 1 (Not Working):** Regular chat â†’ `sendMessage()` â†’ `handle_user_message` â†’ Fallback generator
   - **Path 2 (Exists but Unused):** `MedGemmaChat.tsx` â†’ `/api/medgemma/text/` endpoint â†’ MedGemma AI
   - **Frontend is using Path 1**, not Path 2

3. **Missing MedGemma Integration in Task Handler**
   - The `tasks.py` doesn't know about MedGemma at all
   - It only knows about OpenAI and fallback

---

## Solution: Integrate MedGemma into Task Handler

### Step 1: Modify `chat/tasks.py` to use MedGemma

Replace the `_call_llm_safe()` function to call MedGemma instead of OpenAI:

```python
# In tasks.py, update imports:
from .medgemma_client import get_medgemma_client

def _call_llm_safe(user_id: int, user_text: str, history: List[dict]) -> str:
    """
    Wrapper that calls MedGemma AI for medical responses.
    Falls back to fallback generator if MedGemma unavailable.
    """
    try:
        client = get_medgemma_client()
        
        # Check if MedGemma server is available
        if not client.is_available():
            logger.warning("MedGemma server not available; using fallback.")
            return generate_bot_response_fallback(user_text, {})
        
        # Convert history to MedGemma format
        conversation_history = [
            {"role": "user" if h.get("sender") == "user" else "assistant", 
             "content": h.get("text", "")}
            for h in history
        ]
        
        # Call MedGemma
        response = client.analyze_text(
            message=user_text,
            conversation_history=conversation_history if conversation_history else None,
            temperature=0.7,
            max_tokens=1000
        )
        
        return response if response else generate_bot_response_fallback(user_text, {})
        
    except Exception as e:
        logger.exception(f"MedGemma call failed: {e}")
        return generate_bot_response_fallback(user_text, {})
```

### Step 2: Update the `USE_LLM` Check

Modify the initial LLM availability check:

```python
# Replace this:
# USE_LLM = getattr(settings, "USE_LLM", True) and AI_AVAILABLE

# With this:
USE_LLM = True  # Always enabled - we'll use MedGemma or fallback
```

---

## Implementation Instructions

### File: `chat/tasks.py`

1. **Add import at the top (around line 33):**
   ```python
   from .medgemma_client import get_medgemma_client
   ```

2. **Replace the `_call_llm_safe()` function (lines 178-207)** with the new implementation above

3. **Update the `USE_LLM` flag** (around line 60) to always be `True`

---

## Expected Behavior After Fix

1. User sends message in chat
2. Message saved to database
3. Background task triggered (`handle_user_message`)
4. Task calls `_call_llm_safe()`
5. `_call_llm_safe()` calls **MedGemma AI** (if available)
6. MedGemma returns medical response
7. Response saved as bot message
8. Frontend refetches messages and displays AI response

---

## Testing the Fix

### Prerequisites
Ensure MedGemma server is running:
```bash
ollama serve medgemma-4b
# or whatever your MedGemma setup is
```

### Test Steps
1. Open chat at `http://localhost:3000`
2. Send a message: "What are symptoms of diabetes?"
3. Wait 2-3 seconds
4. Should see AI response from MedGemma, not from fallback

### How to Verify It's Using MedGemma
- Check backend logs for: `"Calling MedGemma for medical analysis"`
- If fallback is used, you'll see: `"MedGemma server not available; using fallback"`

---

## Alternative: Update Frontend to Use MedGemma Endpoint Directly

If you prefer the frontend to call MedGemma directly instead of through the regular chat:

### Current Flow (After Fix Above)
```
Frontend Chat Input 
  â†’ sendMessage() 
  â†’ Backend creates user message 
  â†’ Celery task 
  â†’ MedGemma AI 
  â†’ Creates bot message
  â†’ Frontend polls for new messages
```

### Alternative Flow (More Direct)
```
Frontend Chat Input 
  â†’ Call /api/medgemma/text/ directly
  â†’ Get response immediately
  â†’ Create messages manually
```

This would require modifying `frontend/src/lib/store.ts` to call MedGemma endpoint.

---

## Configuration Check

Verify your Django settings (`mhchat_proj/settings.py`) has:

```python
# MedGemma Configuration
MEDGEMMA_BASE_URL = 'http://localhost:8080'  # or your MedGemma server
MEDGEMMA_MODEL = 'medgemma-4b'
MEDGEMMA_TIMEOUT = 60

# Celery Configuration (for async tasks)
CELERY_BROKER_URL = 'redis://localhost:6379/0'  # or your Redis URL
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'

# Disable OpenAI requirement
OPENAI_API_KEY = None  # This is optional
```

---

## Next Steps

1. âœ… Apply the fix to `chat/tasks.py`
2. âœ… Ensure MedGemma server is running on port 8080
3. âœ… Restart Django server: `python manage.py runserver`
4. âœ… Test the chat with a medical question
5. âœ… Check logs for success/failure

If you need help implementing any of these changes, let me know!
